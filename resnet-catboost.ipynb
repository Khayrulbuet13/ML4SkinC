{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc6f40",
   "metadata": {},
   "source": [
    "# Prediction from resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b17cf8a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-16T03:58:20.382793Z",
     "iopub.status.busy": "2024-07-16T03:58:20.382514Z",
     "iopub.status.idle": "2024-07-16T03:58:29.740038Z",
     "shell.execute_reply": "2024-07-16T03:58:29.739212Z"
    },
    "papermill": {
     "duration": 9.364053,
     "end_time": "2024-07-16T03:58:29.742476",
     "exception": false,
     "start_time": "2024-07-16T03:58:20.378423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Model loaded from checkpoint/13 July 13:46-resnet18-VS_loss-unbalanced_datasetv2.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "kaggle = False\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_file, transform=None):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        self.transform = transform\n",
    "        self.file = h5py.File(hdf5_file, 'r')\n",
    "        self.keys = list(self.file.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.keys[idx]\n",
    "        image_data = self.file[image_name][()]\n",
    "        image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image_name\n",
    "\n",
    "    def close(self):\n",
    "        self.file.close()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "if kaggle:\n",
    "    hdf5_file = '/kaggle/input/isic-2024-challenge/test-image.hdf5'\n",
    "    model_saved_path = '/kaggle/input/resnet18-vsloss/resnet18_vsloss_v2.pt'\n",
    "else:\n",
    "    hdf5_file = 'dataset/dump/train-image.hdf5'\n",
    "    model_saved_path = 'checkpoint/13 July 13:46-resnet18-VS_loss-unbalanced_datasetv2.pt'\n",
    "\n",
    "\n",
    "dataset = HDF5Dataset(hdf5_file=hdf5_file, transform=transform)\n",
    "test_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model = models.resnet18()\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "if os.path.exists(model_saved_path):\n",
    "    model.load_state_dict(torch.load(model_saved_path, map_location=device))\n",
    "    logging.info(f'Model loaded from {model_saved_path}')\n",
    "else:\n",
    "    logging.info('Model not found')\n",
    "\n",
    "model.eval()\n",
    "resnet_predictions, image_ids = [], []\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        resnet_predictions.extend(probabilities[:, 1].cpu().numpy())\n",
    "        image_ids.extend(ids)\n",
    "        count += 1\n",
    "        if count >= 10:\n",
    "            break\n",
    "\n",
    "dataset.close()\n",
    "\n",
    "# Ensure predictions are not zero unless expected by checking and handling zero values\n",
    "resnet_predictions = [max(pred, 1e-5) for pred in resnet_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab311006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015670</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015845</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015864</td>\n",
       "      <td>3.044883e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0015902</td>\n",
       "      <td>7.689489e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0024200</td>\n",
       "      <td>1.401298e-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>ISIC_0088838</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>ISIC_0088843</td>\n",
       "      <td>1.359260e-43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>ISIC_0088870</td>\n",
       "      <td>3.326699e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>ISIC_0088915</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>ISIC_0088930</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          isic_id        target\n",
       "0    ISIC_0015670  0.000000e+00\n",
       "1    ISIC_0015845  0.000000e+00\n",
       "2    ISIC_0015864  3.044883e-26\n",
       "3    ISIC_0015902  7.689489e-04\n",
       "4    ISIC_0024200  1.401298e-45\n",
       "..            ...           ...\n",
       "635  ISIC_0088838  0.000000e+00\n",
       "636  ISIC_0088843  1.359260e-43\n",
       "637  ISIC_0088870  3.326699e-15\n",
       "638  ISIC_0088915  0.000000e+00\n",
       "639  ISIC_0088930  0.000000e+00\n",
       "\n",
       "[640 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_submission(test_ids, predictions, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'isic_id': test_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f'Submission saved to {filename}')\n",
    "    return submission\n",
    "\n",
    "# Assuming you have the test IDs ready\n",
    "create_submission(image_ids, resnet_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3639d321",
   "metadata": {},
   "source": [
    "# Prediction from catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be60cf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T03:58:29.749606Z",
     "iopub.status.busy": "2024-07-16T03:58:29.749189Z",
     "iopub.status.idle": "2024-07-16T04:06:44.772404Z",
     "shell.execute_reply": "2024-07-16T04:06:44.771432Z"
    },
    "papermill": {
     "duration": 495.029248,
     "end_time": "2024-07-16T04:06:44.774600",
     "exception": false,
     "start_time": "2024-07-16T03:58:29.745352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 168.29 MB\n",
      "Memory usage after optimization is: 109.77\n",
      "Decreased by 34.8%\n",
      "Memory usage of dataframe is 0.00 MB\n",
      "Memory usage after optimization is: 0.00\n",
      "Decreased by 35.2%\n",
      "Processing column: age_approx with KNN\n",
      "0:\ttest: 0.7846844\tbest: 0.7846844 (0)\ttotal: 145ms\tremaining: 2m 25s\n",
      "100:\ttest: 0.9473169\tbest: 0.9476601 (86)\ttotal: 7.49s\tremaining: 1m 6s\n",
      "200:\ttest: 0.9517468\tbest: 0.9524908 (162)\ttotal: 14.9s\tremaining: 59.3s\n",
      "300:\ttest: 0.9537625\tbest: 0.9540731 (268)\ttotal: 22.6s\tremaining: 52.4s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9540730814\n",
      "bestIteration = 268\n",
      "\n",
      "Shrink model to first 269 iterations.\n",
      "0:\ttest: 0.7051942\tbest: 0.7051942 (0)\ttotal: 79.5ms\tremaining: 1m 19s\n",
      "100:\ttest: 0.9400225\tbest: 0.9401430 (98)\ttotal: 7.13s\tremaining: 1m 3s\n",
      "200:\ttest: 0.9467441\tbest: 0.9469202 (184)\ttotal: 13.9s\tremaining: 55.2s\n",
      "300:\ttest: 0.9463501\tbest: 0.9473208 (250)\ttotal: 20.7s\tremaining: 48s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.947320807\n",
      "bestIteration = 250\n",
      "\n",
      "Shrink model to first 251 iterations.\n",
      "0:\ttest: 0.8245523\tbest: 0.8245523 (0)\ttotal: 83.3ms\tremaining: 1m 23s\n",
      "100:\ttest: 0.9418985\tbest: 0.9418985 (100)\ttotal: 6.98s\tremaining: 1m 2s\n",
      "200:\ttest: 0.9437126\tbest: 0.9464300 (154)\ttotal: 13.8s\tremaining: 55s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9464300411\n",
      "bestIteration = 154\n",
      "\n",
      "Shrink model to first 155 iterations.\n",
      "0:\ttest: 0.7348521\tbest: 0.7348521 (0)\ttotal: 78.8ms\tremaining: 1m 18s\n",
      "100:\ttest: 0.9353245\tbest: 0.9355837 (92)\ttotal: 7.09s\tremaining: 1m 3s\n",
      "200:\ttest: 0.9397486\tbest: 0.9403368 (189)\ttotal: 14.1s\tremaining: 56.1s\n",
      "300:\ttest: 0.9419896\tbest: 0.9419896 (300)\ttotal: 21s\tremaining: 48.8s\n",
      "400:\ttest: 0.9430229\tbest: 0.9431575 (380)\ttotal: 27.9s\tremaining: 41.7s\n",
      "500:\ttest: 0.9440435\tbest: 0.9441847 (447)\ttotal: 34.8s\tremaining: 34.6s\n",
      "600:\ttest: 0.9452474\tbest: 0.9452474 (600)\ttotal: 41.7s\tremaining: 27.7s\n",
      "700:\ttest: 0.9451665\tbest: 0.9462907 (664)\ttotal: 48.7s\tremaining: 20.8s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9462907157\n",
      "bestIteration = 664\n",
      "\n",
      "Shrink model to first 665 iterations.\n",
      "0:\ttest: 0.6460183\tbest: 0.6460183 (0)\ttotal: 78.4ms\tremaining: 1m 18s\n",
      "100:\ttest: 0.9449166\tbest: 0.9452263 (83)\ttotal: 7.08s\tremaining: 1m 3s\n",
      "200:\ttest: 0.9448735\tbest: 0.9467637 (176)\ttotal: 14.1s\tremaining: 56s\n",
      "300:\ttest: 0.9455620\tbest: 0.9468509 (257)\ttotal: 21s\tremaining: 48.7s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.9468508604\n",
      "bestIteration = 257\n",
      "\n",
      "Shrink model to first 258 iterations.\n",
      "Average pAUC score: 0.1784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############## Catboost ####################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load data from csv files\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f}'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100*(start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data( Kaggle=False, debug=False):\n",
    "    if Kaggle:\n",
    "        train_meta = pd.read_csv('/kaggle/input/isic-2024-challenge/train-metadata.csv')\n",
    "        test_meta = pd.read_csv('/kaggle/input/isic-2024-challenge/test-metadata.csv')\n",
    "    else:\n",
    "        train_meta = pd.read_csv('dataset/dump/train-metadata.csv')\n",
    "        test_meta = pd.read_csv('dataset/dump/test-metadata.csv')\n",
    "    if debug:\n",
    "        train_meta = train_meta[:80*1000]\n",
    "    return reduce_mem_usage(train_meta), reduce_mem_usage(test_meta)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "def preprocess_and_impute_data(df, columns_to_drop):\n",
    "    # Drop specified columns safely\n",
    "    df = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Specific handling for the 'age_approx' column using KNN\n",
    "    if 'age_approx' in df.columns and df['age_approx'].isnull().any():\n",
    "        print('Processing column: age_approx with KNN')\n",
    "        knn_imputer = KNNImputer(n_neighbors=5)\n",
    "        age_approx_values = df[['age_approx']]\n",
    "        df['age_approx'] = knn_imputer.fit_transform(age_approx_values).ravel()  # Reshape to 1D array\n",
    "\n",
    "    # Handle missing data for other numeric columns using mean imputation\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'age_approx' in numeric_cols:\n",
    "        numeric_cols.remove('age_approx')\n",
    "    for col in numeric_cols:\n",
    "        if df[col].isnull().any():\n",
    "            print(f'Processing column: {col} with mean imputation')\n",
    "            mean_imputer = SimpleImputer(strategy='mean')\n",
    "            df[col] = mean_imputer.fit_transform(df[[col]]).ravel()  # Reshape to 1D array\n",
    "    \n",
    "    # Handle missing data for categorical columns using median imputation\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df[col].isnull().any():\n",
    "            print(f'Processing column: {col} with median imputation')\n",
    "            median_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df[col] = median_imputer.fit_transform(df[[col]]).ravel()  # Reshape to 1D array\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"lesion_size_ratio\"] = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n",
    "    df[\"lesion_shape_index\"] = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n",
    "    df[\"hue_contrast\"] = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n",
    "    df[\"luminance_contrast\"] = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n",
    "    df[\"lesion_color_difference\"] = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n",
    "    df[\"border_complexity\"] = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"color_uniformity\"] = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n",
    "    df[\"3d_position_distance\"] = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n",
    "    df[\"perimeter_to_area_ratio\"] = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n",
    "    df[\"lesion_visibility_score\"] = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n",
    "    df[\"symmetry_border_consistency\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n",
    "    df[\"color_consistency\"] = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n",
    "    df[\"size_age_interaction\"] = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n",
    "    df[\"hue_color_std_interaction\"] = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"lesion_severity_index\"] = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n",
    "    df[\"shape_complexity_index\"] = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n",
    "    df[\"color_contrast_index\"] = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n",
    "    df[\"log_lesion_area\"] = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n",
    "    df[\"normalized_lesion_size\"] = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n",
    "    df[\"mean_hue_difference\"] = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n",
    "    df[\"std_dev_contrast\"] = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n",
    "    df[\"color_shape_composite_index\"] = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n",
    "    df[\"3d_lesion_orientation\"] = np.arctan2(df[\"tbp_lv_y\"], df[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"] = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n",
    "    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n",
    "    df[\"comprehensive_lesion_index\"] = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n",
    "    return df\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def calculate_pauc(y_true, y_scores, tpr_threshold=0.8):\n",
    "    # Calculate pAUC using sklearn's roc_auc_score with max_fpr\n",
    "    partial_auc_scaled = roc_auc_score(y_true, y_scores, max_fpr=tpr_threshold)\n",
    "\n",
    "    # Scale from [0.5, 1.0] to [0.0, 0.2]\n",
    "    partial_auc = (partial_auc_scaled - 0.5) * 0.4\n",
    "    return partial_auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(train_df, categorical_columns):\n",
    "    # Assuming 'target' is the column name for your target variable\n",
    "    X = train_df.drop(columns=['target'])\n",
    "    y = train_df['target']\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    pauc_scores = []\n",
    "    models = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        model = CatBoostClassifier(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.1,\n",
    "            depth=6,\n",
    "            cat_features=categorical_columns,\n",
    "            eval_metric='AUC',\n",
    "            random_seed=42,\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=100\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100)\n",
    "        \n",
    "        test_pred = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        models.append(model)\n",
    "\n",
    "        pauc = calculate_pauc(y_test, test_pred)\n",
    "        pauc_scores.append(pauc)\n",
    "\n",
    "    print(f'Average pAUC score: {np.mean(pauc_scores):.4f}')\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict(models, test_meta_df_le):\n",
    "    submit_score = []\n",
    "    for model in models:\n",
    "        pred_ = model.predict_proba(test_meta_df_le)[:, 1]\n",
    "        submit_score.append(pred_)\n",
    "    submit_pred = np.mean(submit_score, axis=0)\n",
    "    return submit_pred\n",
    "\n",
    "def create_submission(test_id, submit_pred, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'isic_id': test_id,\n",
    "        'target': submit_pred\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    return submission\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def fit_and_save_encoders(train_df, categorical_columns, file_path='encoders.joblib'):\n",
    "    \"\"\"\n",
    "    Fits LabelEncoders for each categorical column in train_df and saves the encoders.\n",
    "    \n",
    "    :param train_df: DataFrame containing the training data.\n",
    "    :param categorical_columns: List of column names that are categorical.\n",
    "    :param file_path: File path to save the encoders.\n",
    "    :return: DataFrame with encoded categorical columns.\n",
    "    \"\"\"\n",
    "    encoders = {}\n",
    "    for column in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit the encoder and transform the data\n",
    "        train_df[column] = le.fit_transform(train_df[column].astype(str))\n",
    "        # Save the encoder in a dictionary\n",
    "        encoders[column] = le\n",
    "    # Save all encoders to disk\n",
    "    joblib.dump(encoders, file_path)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "\n",
    "def load_and_apply_encoders(test_df, categorical_columns, file_path='encoders.joblib'):\n",
    "    \"\"\"\n",
    "    Loads encoders and applies them to the test_df, handling unseen categories.\n",
    "    \n",
    "    :param test_df: DataFrame containing the testing data.\n",
    "    :param categorical_columns: List of column names that are categorical.\n",
    "    :param file_path: File path where the encoders are saved.\n",
    "    :return: DataFrame with encoded categorical columns.\n",
    "    \"\"\"\n",
    "    # Load the saved encoders\n",
    "    encoders = joblib.load(file_path)\n",
    "    for column in categorical_columns:\n",
    "        le = encoders[column]\n",
    "        # Handle unseen categories by using 'transform' method and custom handling for unknown categories\n",
    "        test_df[column] = test_df[column].map(lambda s: le.transform([s])[0] if s in le.classes_ else -1)\n",
    "    return test_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df, test_df = load_data(Kaggle=False, debug=False)\n",
    "\n",
    "test_ids = test_df['isic_id']\n",
    "\n",
    "\n",
    "# Drop columns that are not needed and handle missing data\n",
    "missin_in_test = ['iddx_3', 'iddx_full', 'iddx_2', 'mel_mitotic_index', \n",
    "                  'iddx_1', 'lesion_id', 'tbp_lv_dnn_lesion_confidence', \n",
    "                  'iddx_5', 'iddx_4', 'mel_thick_mm']\n",
    "columns_to_drop = missin_in_test + ['isic_id', 'patient_id', 'sex', 'anatom_site_general', \n",
    "                    'image_type', 'tbp_tile_type', 'attribution', 'copyright_license']\n",
    "test_df = preprocess_and_impute_data(test_df, columns_to_drop)\n",
    "train_df = preprocess_and_impute_data(train_df, columns_to_drop)\n",
    "\n",
    "\n",
    "# Label encode categorical columns\n",
    "categorical_cols = [ 'tbp_lv_location', 'tbp_lv_location_simple']\n",
    "# Fit encoders to the training data and save them\n",
    "train_df = fit_and_save_encoders(train_df, categorical_cols)\n",
    "\n",
    "# Load the encoders and apply them to the testing data\n",
    "test_df = load_and_apply_encoders(test_df, categorical_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model and make predictions\n",
    "models = train_model(train_df, categorical_columns=categorical_cols)\n",
    "catboost_predictions = predict(models, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76afd7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>0.000205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015740</td>\n",
       "      <td>0.000179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0015657  0.000205\n",
       "1  ISIC_0015729  0.000021\n",
       "2  ISIC_0015740  0.000179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate submission file\n",
    "def create_submission(test_ids, predictions, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'isic_id': test_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f'Submission saved to {filename}')\n",
    "    return submission\n",
    "\n",
    "# Assuming you have the test IDs ready\n",
    "create_submission(test_ids, catboost_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df03ce23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-16T04:06:44.785773Z",
     "iopub.status.busy": "2024-07-16T04:06:44.785279Z",
     "iopub.status.idle": "2024-07-16T04:06:44.803667Z",
     "shell.execute_reply": "2024-07-16T04:06:44.802769Z"
    },
    "papermill": {
     "duration": 0.026359,
     "end_time": "2024-07-16T04:06:44.805891",
     "exception": false,
     "start_time": "2024-07-16T04:06:44.779532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>0.352896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015740</td>\n",
       "      <td>0.046168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0015657  0.352896\n",
       "1  ISIC_0015729  0.000006\n",
       "2  ISIC_0015740  0.046168"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############# Ensemble ####################\n",
    "ensemble_predictions = (np.array(resnet_predictions) + np.array(catboost_predictions)) / 2  \n",
    "\n",
    "\n",
    "# Generate submission file\n",
    "def create_submission(test_ids, predictions, filename='submission.csv'):\n",
    "    submission = pd.DataFrame({\n",
    "        'isic_id': test_ids,\n",
    "        'target': predictions\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f'Submission saved to {filename}')\n",
    "    return submission\n",
    "\n",
    "# Assuming you have the test IDs ready\n",
    "create_submission(test_ids, ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512ba8a",
   "metadata": {
    "papermill": {
     "duration": 0.004758,
     "end_time": "2024-07-16T04:06:44.815938",
     "exception": false,
     "start_time": "2024-07-16T04:06:44.811180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9094797,
     "sourceId": 63056,
     "sourceType": "competition"
    },
    {
     "datasetId": 5393889,
     "sourceId": 8961416,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 509.273695,
   "end_time": "2024-07-16T04:06:46.644681",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-16T03:58:17.370986",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
